{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import streamlit as st\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import StreamlitChatMessageHistory\n",
    "from llm import llm\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pinecone\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "AZURE_OPENAI_API_KEY = os.environ.get('AZURE_OPENAI_API_KEY')\n",
    "AZURE_OPENAI_DEPLOYMENT_NAME = os.environ.get('AZURE_OPENAI_DEPLOYMENT_NAME')\n",
    "AZURE_ENDPOINT = os.environ.get('AZURE_ENDPOINT')\n",
    "API_VERSION = os.environ.get('API_VERSION_LLM')\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=AZURE_ENDPOINT,\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    azure_deployment=AZURE_OPENAI_DEPLOYMENT_NAME,\n",
    "    api_version=API_VERSION,\n",
    "    temperature=0,\n",
    "    streaming=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "RAG_TEMPLATE = \"\"\"\n",
    "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Answer the following question:\n",
    "\n",
    "{question}\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(RAG_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatAgent:\n",
    "    def __init__(self):\n",
    "        self.history = StreamlitChatMessageHistory(key=\"chat_history\")\n",
    "        self.embedding_model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')  # Sentence Transformer for embeddings\n",
    "        pinecone.init(api_key=os.environ.get('PINECONE_API_KEY'))\n",
    "        self.index = pinecone.Index('test-git')\n",
    "        self.chain = self.setup_chain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_chain(self):\n",
    "        \"\"\"Set up a LangChain with message history.\"\"\"\n",
    "        return RunnableWithMessageHistory(\n",
    "            prompt_template | llm,\n",
    "            lambda session_id: self.history,\n",
    "            input_messages_key=\"question\",\n",
    "            history_messages_key=\"history\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_messages(self):\n",
    "        \"\"\"Display chat messages.\"\"\"\n",
    "        if len(self.history.messages) == 0:\n",
    "            self.history.add_ai_message(\"How can I assist you?\")\n",
    "        for msg in self.history.messages:\n",
    "            st.chat_message(msg.type).write(msg.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_retrieved_content_for_prompt(self, search_results):\n",
    "        \"\"\"Format the retrieved content for the prompt.\"\"\"\n",
    "        context = \"\"\n",
    "        for match in search_results[\"results\"][0][\"matches\"]:\n",
    "            title = match[\"metadata\"][\"title\"]\n",
    "            url = match[\"metadata\"][\"url\"]\n",
    "            text = match[\"metadata\"][\"text\"]\n",
    "            context += f\"Title: {title}\\nURL: {url}\\nText: {text}\\n\\n\"\n",
    "        return context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(self, question):\n",
    "        \"\"\"Retrieve documents from Pinecone using a question embedding.\"\"\"\n",
    "        query_embedding = self.embedding_model.encode(question).tolist()\n",
    "        print(len(query_embedding))\n",
    "        search_results = self.index.query(queries=[query_embedding], top_k=5, include_metadata=True)\n",
    "        return search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_from_llm(self, context, question):\n",
    "        \"\"\"Generate answer using the LLM with given context and question.\"\"\"\n",
    "        response = self.chain.invoke({\"context\": context, \"question\": question})\n",
    "        return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_conversation(self):\n",
    "        \"\"\"Start the chat conversation.\"\"\"\n",
    "        self.display_messages()\n",
    "        user_question = st.chat_input(placeholder=\"Ask me anything!\")\n",
    "        if user_question:\n",
    "            st.chat_message(\"human\").write(user_question)\n",
    "\n",
    "            # Retrieve relevant documents from Pinecone\n",
    "            search_results = self.retrieve_documents(user_question)\n",
    "            context = self.format_retrieved_content_for_prompt(search_results)\n",
    "            \n",
    "            # Get answer from LLM\n",
    "            answer = self.get_answer_from_llm(context, user_question)\n",
    "\n",
    "            # Display answer and sources\n",
    "            st.chat_message(\"ai\").write(answer)\n",
    "            \n",
    "            # Display sources separately with clickable links\n",
    "            st.write(\"#### Sources:\")\n",
    "            for match in search_results[\"results\"][0][\"matches\"]:\n",
    "                title = match[\"metadata\"][\"title\"]\n",
    "                url = match[\"metadata\"][\"url\"]\n",
    "                st.write(f\"**[{title}]({url})**\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
